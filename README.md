# SRLP Thesis Evaluation Pipeline

A comprehensive evaluation system for the Strategic Reasoning and Learning Progression (SRLP) framework against established baseline strategies across multiple AI providers.

## üéØ Overview

This pipeline evaluates **4 reasoning strategies** across **5 domains** using **3 AI providers**:

- **Strategies**: SRLP, Chain-of-Thought (CoT), Tree-of-Thoughts (ToT), ReAct
- **Providers**: OpenAI GPT-4, Anthropic Claude-3, Google Gemini  
- **Domains**: Travel Planning, Software Project, Event Organization, Research Study, Business Launch
- **Total Experiments**: 5,400 (450 scenarios √ó 4 strategies √ó 3 providers)

## üöÄ Quick Start

### 1. Installation

```bash
pip install --break-system-packages -r requirements.txt
```

### 2. Secure API Key Setup

‚ö†Ô∏è **IMPORTANT**: API keys are now stored securely in environment variables.

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env with your actual API keys (NEVER commit this file!)
nano .env  # or your preferred editor
```

Required API Keys (add to `.env`):
- **OpenAI**: Get from https://platform.openai.com/api-keys
- **Anthropic**: Get from https://console.anthropic.com/
- **Gemini**: Get from https://ai.google.dev/

Example `.env` format:
```bash
OPENAI_API_KEY=sk-proj-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
ANTHROPIC_API_KEY=sk-ant-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
GEMINI_API_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```

üîí **Security Notes**: 
- The `.env` file is automatically ignored by Git
- Never commit API keys to version control
- Use `.env.example` as a template only
- If keys are missing, you'll get a clear error message

### 3. Run Complete Evaluation

```bash
python run_evaluation.py \
  --providers gpt4,claude3,gemini \
  --strategies srlp,cot,tot,react \
  --async \
  --workers 8 \
  --batch-size 300 \
  --log-level INFO \
  --resume-from auto
```

### 4. Dry Run (Validation)

```bash
python run_evaluation.py --dry-run
```

**Expected Output:**
```
Providers: 3 (gpt4, claude3, gemini)
Strategies: 4 (srlp, cot, tot, react)
Domains: 5
Scenarios: 450
Total experiments: 5400
```

### 5. Generate Artifacts from Existing Results

```bash
# Generate LaTeX tables and figures from evaluation results
python generate_artifacts.py results_full/evaluation_results.csv

# Custom output directory
python generate_artifacts.py results_full/evaluation_results.csv --output artifacts_custom
```

### 6. Verify Outputs

```bash
python run_evaluation.py --verify-outputs
```

## üìä Evaluation Metrics

The pipeline implements four key metrics:

- **PQS (Plan Quality Score)**: 0-100 scale measuring solution completeness and quality
- **SCCS (Self-Check Confidence Score)**: 0-100 scale measuring confidence indicators  
- **IIR (Iteration Improvement Rate)**: 0-100 scale measuring iterative refinement
- **CEM (Cost Efficiency Metric)**: 0-100 scale measuring resource optimization

## üß† Strategy Implementations

### SRLP (Self-Refinement for LLM Planners)
4-stage process: Plan Generation ‚Üí Self-Assessment ‚Üí Refinement ‚Üí Quality Assurance

### CoT (Chain-of-Thought)
Based on Wei et al. (2022), systematic step-by-step reasoning

### ToT (Tree-of-Thoughts)  
Based on Yao et al. (2024), branching exploration with evaluation

### ReAct (Reasoning and Acting)
Based on Yao et al. (2022), interleaved reasoning-action-observation cycles

## üìÅ Project Structure

```
SRLP-thesis-/
‚îú‚îÄ‚îÄ src/                    # Core modules
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ providers.py       # AI provider clients  
‚îÇ   ‚îú‚îÄ‚îÄ strategies.py      # Strategy implementations
‚îÇ   ‚îú‚îÄ‚îÄ scenarios.py       # Scenario generation
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py         # Metrics calculation
‚îÇ   ‚îî‚îÄ‚îÄ outputs.py         # Output generation
‚îú‚îÄ‚îÄ run_evaluation.py      # Main entry point
‚îú‚îÄ‚îÄ requirements.txt       # Dependencies
‚îú‚îÄ‚îÄ README.md             # This file
‚îî‚îÄ‚îÄ results_full/         # Output directory
```

## üé® Artifacts Generation

The pipeline includes a sophisticated artifacts generation system that creates publication-ready outputs:

### LaTeX Tables
- **Table 4.1**: PQS scores by strategy with statistical measures
- **Table 4.2**: Provider performance (time, cost, tokens, success rate)  
- **Table 4.3**: SRLP performance gains by domain vs baselines
- **Table 4.4**: PQS performance by complexity level and strategy
- **Table 4.5**: Strategic cognitive capability scores by dimension

### High-Quality Figures (300 DPI PNG)
- **Figure 4.1**: PQS distribution by strategy (box plots)
- **Figure 4.2**: Provider time and cost analysis (bar charts)
- **Figure 4.3**: SRLP gains by domain (grouped bar chart)
- **Figure 4.4**: PQS by complexity level (grouped bar chart)
- **Figure 4.5**: Strategic cognitive capabilities (radar chart)

### Key Features
- **Headless Operation**: Uses `MPLBACKEND=Agg` for server environments
- **Real Data Processing**: Computes actual statistics from evaluation results
- **Publication Ready**: Professional formatting for academic papers
- **Standalone Generation**: Can process existing CSV files independently

## üîß Configuration

API keys are securely stored in `src/config.py`. The system supports:

- **Async Processing**: Concurrent execution with configurable workers
- **Batching**: Configurable batch sizes for optimal performance
- **Checkpointing**: Resume capability for long-running evaluations  
- **Deterministic Results**: Fixed seeds for reproducible experiments

## üìà Outputs

All results are saved to `results_full/`:

### Data Files
- `evaluation_results.csv` - Raw tabular results
- `detailed_results.json` - Complete results with metadata
- `scenarios.json` - Generated scenarios

### LaTeX Tables (Ready for Chapter 4)
- `table_4_1_pqs_by_strategy.tex` - PQS scores by strategy and provider
- `table_4_2_provider_time_cost.tex` - Provider performance metrics
- `table_4_3_domain_gains.tex` - SRLP gains by domain
- `table_4_4_pqs_by_complexity.tex` - Performance by complexity level
- `table_4_5_sccs_by_dimension.tex` - Cognitive capability scores

### Figures (PNG + PDF, 300 DPI)
- `figure_4_1_pqs_by_strategy` - PQS distribution by strategy
- `figure_4_2_provider_time_cost` - Time and cost analysis
- `figure_4_3_pqs_gain_by_domain` - Domain-specific gains
- `figure_4_4_pqs_by_complexity` - Complexity analysis
- `figure_4_5_sccs_by_dimension` - Cognitive dimensions

### Summary Report
- `RUN_SUMMARY.md` - Executive summary with key findings

## üéõÔ∏è Command Line Options

| Option | Default | Description |
|--------|---------|-------------|
| `--providers` | `gpt4,claude3,gemini` | Comma-separated list of AI providers |
| `--strategies` | `srlp,cot,tot,react` | Comma-separated list of strategies |
| `--workers` | `8` | Number of concurrent workers |
| `--batch-size` | `300` | Tasks per batch |
| `--log-level` | `INFO` | Logging verbosity |
| `--resume-from` | `None` | Resume from checkpoint (`auto` or file path) |
| `--dry-run` | `False` | Validation mode only |

## üîç Key Features

### Real Baseline Implementations
All baseline strategies (CoT, ToT, ReAct) use prompts and techniques from the original research papers, not placeholders.

### Comprehensive Evaluation  
- 450 scenarios across 5 diverse domains
- Exactly 90 scenarios per domain with deterministic generation
- Complex scenarios with varying difficulty levels

### Robust Execution
- Exponential backoff retry logic for API failures
- Real-time progress monitoring
- Automatic checkpointing for resume capability
- Comprehensive error handling

### Publication-Ready Outputs
- LaTeX tables formatted for academic papers
- High-resolution figures (300 DPI)
- Statistical analysis and comparisons
- Professional summary reports

## üìä Expected Results

The evaluation produces valid numerical comparisons showing:

- **SRLP vs CoT**: Performance gains across domains
- **SRLP vs ToT**: Efficiency vs thoroughness trade-offs  
- **SRLP vs ReAct**: Action-oriented vs planning-oriented approaches
- **Provider Analysis**: Cost, speed, and quality comparisons
- **Domain Analysis**: Strategy effectiveness by problem type

## ‚è±Ô∏è Estimated Runtime

- **Full Evaluation**: 6-8 hours (5,400 experiments)
- **Single Provider**: 2-3 hours (1,800 experiments)
- **Dry Run**: < 1 minute (validation only)

Times vary based on API response latencies and retry frequency.

## üõ†Ô∏è Troubleshooting

### Common Issues

1. **API Rate Limits**: Reduce `--workers` or increase `--batch-size`
2. **Memory Issues**: Decrease `--batch-size` for large-scale runs  
3. **Network Timeouts**: Pipeline automatically retries with backoff
4. **Dependency Conflicts**: Use `--break-system-packages` flag

### Resume Capability

```bash
# Auto-resume from latest checkpoint
python run_evaluation.py --resume-from auto

# Resume from specific checkpoint  
python run_evaluation.py --resume-from results_full/checkpoint_latest.json
```

## ‚úÖ Validation

The pipeline includes comprehensive validation:

- ‚úÖ Exactly 450 scenarios (90 per domain)
- ‚úÖ All 4 strategies with real implementations
- ‚úÖ All 3 providers properly integrated
- ‚úÖ 5,400 total experiments enumerated
- ‚úÖ Valid numerical results (no NaN/zeros)
- ‚úÖ All output artifacts generated

## üìñ Citation

```bibtex
@misc{srlp_evaluation_2024,
  title={Strategic Reasoning and Learning Progression: A Comprehensive Evaluation Framework},
  author={[Your Name]},
  year={2024},
  note={Thesis Evaluation Pipeline - 5,400 experiments across 3 AI providers}
}
```

---

**Ready for thesis Chapter 4 with complete LaTeX tables and figures!** üéì
